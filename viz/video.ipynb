{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55374b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import all required libraries for video generation pipeline\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import viser.transforms as tf\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from torchaudio.transforms import GriffinLim\n",
    "from scipy.signal import fftconvolve\n",
    "from nerfstudio.viewer.viewer import VISER_NERFSTUDIO_SCALE_RATIO\n",
    "from nerfstudio.field_components.spatial_distortions import SceneContraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e593b9",
   "metadata": {},
   "source": [
    "# NeRAF Video Generation Pipeline\n",
    "\n",
    "This notebook provides a complete workflow to generate multi-modal audio-visual videos from NeRAF models. It includes:\n",
    "1. Trajectory creation \n",
    "2. Audio synthesis \n",
    "3. Image rendering \n",
    "4. Video assembly combining audio and synchronized visuals\n",
    "5. Trajectory visualization mini-maps\n",
    "\n",
    "**Note:** This example is for the EmptyRoom scene (RAF dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d780d",
   "metadata": {},
   "source": [
    "⚠️ **Important:** The following examples are specific to EmptyRoom. Each room/environment may require different parameter adjustments (offsets, scaling factors, rotation angles, etc.). These should be tuned based on visual inspection in the nerfstudio viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these paths according to your environment\n",
    "# ============================================================================\n",
    "\n",
    "# Dataset and output paths\n",
    "path_dataset = '/path/to/raf_dataset'\n",
    "output_path_traj = '/path/to/output/trajectories_for_videos'\n",
    "output_path_video = '/path/to/output/EmptyRoom_video'\n",
    "\n",
    "# Select the room to process\n",
    "room = os.path.join(path_dataset, \"archived/EmptyRoom_processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21434c7a",
   "metadata": {},
   "source": [
    "## Step 1: Create Trajectory File \n",
    "\n",
    "Extract and process all poses (microphone positions, speaker positions, rotations) from the dataset split to create a camera trajectory for video rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e85d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_poses(files, room):\n",
    "    \"\"\"\n",
    "    Extract poses from dataset files.\n",
    "    \n",
    "    Args:\n",
    "        files: List of recording file names to process\n",
    "        room: Path to the room dataset directory\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - 'rot': Speaker rotations (spherical harmonics representation)\n",
    "            - 'mic_pose': Microphone positions\n",
    "            - 'source_pose': Speaker positions  \n",
    "            - 'rot_degree': Rotation angles in degrees\n",
    "    \"\"\"\n",
    "    mic_poses_list = []\n",
    "    source_poses_list = []\n",
    "    rots_list = []\n",
    "    rot_degrees_list = []\n",
    "    \n",
    "    for file_name in files:\n",
    "        # Load receiver (microphone) and transmitter (speaker) positions\n",
    "        rx_file = os.path.join(room, 'data', file_name, 'rx_pos.txt')\n",
    "        tx_file = os.path.join(room, 'data', file_name, 'tx_pos.txt')\n",
    "        \n",
    "        with open(rx_file, 'r') as f:\n",
    "            rx_lines = f.readlines()\n",
    "            rx_coords = [float(val) for val in rx_lines[0].replace('\\n', '').split(',')]\n",
    "            \n",
    "        with open(tx_file, 'r') as f:\n",
    "            tx_lines = f.readlines()\n",
    "            tx_coords = [float(val) for val in tx_lines[0].replace('\\n', '').split(',')]\n",
    "\n",
    "        # Extract quaternion and positions from transmitter data\n",
    "        quat_xyzw = tx_coords[:4]  # Quaternion in (x, y, z, w) format\n",
    "        speaker_position = tx_coords[4:]\n",
    "        \n",
    "        # Convert quaternion to Euler angles to extract rotation around Y axis\n",
    "        rotation = R.from_quat(quat_xyzw)\n",
    "        view_dir = np.array([1, 0, 0])\n",
    "        speaker_viewdir = rotation.apply(view_dir)\n",
    "        \n",
    "        # Get rotation around Y axis in degrees\n",
    "        euler_angles = rotation.as_euler('yxz', degrees=True)\n",
    "        rotation_degree = np.round(euler_angles[0], decimals=0)\n",
    "        \n",
    "        # Convert to spherical harmonics representation for audio\n",
    "        rad_rotation = np.deg2rad(rotation_degree)\n",
    "        speaker_rotation_sh = np.array([np.cos(rad_rotation), 0, np.sin(rad_rotation)])\n",
    "        speaker_rotation_sh = (speaker_rotation_sh + 1.0) / 2.0  # Normalize to [0, 1]\n",
    "        \n",
    "        # Store poses\n",
    "        mic_poses_list.append(np.array(rx_coords))\n",
    "        source_poses_list.append(np.array(speaker_position))\n",
    "        rots_list.append(speaker_rotation_sh)\n",
    "        rot_degrees_list.append(rotation_degree)\n",
    "    \n",
    "    # Stack all poses\n",
    "    return {\n",
    "        'rot': np.array(rots_list),\n",
    "        'mic_pose': np.array(mic_poses_list),\n",
    "        'source_pose': np.array(source_poses_list),\n",
    "        'rot_degree': np.array(rot_degrees_list)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa29f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train/test split from the dataset\n",
    "split_file = os.path.join(room, 'metadata/data-split.json')\n",
    "with open(split_file, 'r') as f:\n",
    "    split_files = json.load(f)\n",
    "\n",
    "# Combine training and test files to get all recordings\n",
    "split_files_train = split_files['train'][0]\n",
    "split_files_test = split_files['test'][0]\n",
    "split_files_all = split_files_train + split_files_test\n",
    "\n",
    "print(f\"Loaded {len(split_files_all)} recording files from dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9007cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all poses\n",
    "poses = _process_poses(split_files_all, room)\n",
    "aabb = np.array([poses['mic_pose'].min(axis=0), poses['mic_pose'].max(axis=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958276f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract microphone and speaker positions for reference\n",
    "mic_positions = poses[\"mic_pose\"]\n",
    "speaker_positions = poses[\"source_pose\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be548fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Define the trajectory\n",
    "# ============================================================================\n",
    "# This trajectory is handcrafted to explore the room in an interesting way\n",
    "# Adjust these parameters for your specific room layout\n",
    "\n",
    "step = 0.08  # Distance between frames along the trajectory\n",
    "\n",
    "# Start from the left side, scan horizontally  \n",
    "x_trajectory = np.arange(-1.8, -3.2, -step).tolist()\n",
    "y_trajectory = [3] * len(x_trajectory)\n",
    "\n",
    "y_steps = np.arange(3, 0.8, -step).tolist()\n",
    "x_trajectory += [-3.2] * len(y_steps)\n",
    "y_trajectory += y_steps\n",
    "\n",
    "x_steps = np.arange(-3.2, -1.3, step).tolist()\n",
    "x_trajectory += x_steps\n",
    "y_trajectory += [0.8] * len(x_steps)\n",
    "\n",
    "y_steps = np.arange(0.8, -1.5, -step).tolist()\n",
    "x_trajectory += [-1.3] * len(y_steps)\n",
    "y_trajectory += y_steps\n",
    "\n",
    "# Pause at corner\n",
    "x_trajectory += [x_trajectory[-1]] * 20\n",
    "y_trajectory += [y_trajectory[-1]] * 20\n",
    "\n",
    "y_steps = np.arange(-1.5, -3.5, -step).tolist()\n",
    "x_trajectory += [-1.3] * len(y_steps)\n",
    "y_trajectory += y_steps\n",
    "\n",
    "x_steps = np.arange(-1.3, 0.5, step).tolist()\n",
    "x_trajectory += x_steps\n",
    "y_trajectory += [-3.5] * len(x_steps)\n",
    "\n",
    "y_steps = np.arange(-3.5, -0.6, step).tolist()\n",
    "x_trajectory += [0.5] * len(y_steps)\n",
    "y_trajectory += y_steps\n",
    "\n",
    "x_steps = np.arange(0.5, -1, -step).tolist()\n",
    "x_trajectory += x_steps\n",
    "y_trajectory += [-0.6] * len(x_steps)\n",
    "\n",
    "# Pause again\n",
    "x_trajectory += [x_trajectory[-1]] * 20\n",
    "y_trajectory += [y_trajectory[-1]] * 20\n",
    "\n",
    "x_steps = np.arange(-1, -2.6, -step).tolist()\n",
    "x_trajectory += x_steps\n",
    "y_trajectory += [-0.6] * len(x_steps)\n",
    "\n",
    "y_steps = np.arange(-0.6, -3.5, -step).tolist()\n",
    "x_trajectory += [-2.6] * len(y_steps)\n",
    "y_trajectory += y_steps\n",
    "\n",
    "x_steps = np.arange(-2.6, -2.4, step).tolist()\n",
    "y_steps = np.arange(-3.5, -3.3, step).tolist()\n",
    "x_trajectory += x_steps\n",
    "y_trajectory += y_steps\n",
    "\n",
    "y_steps = np.arange(-3.3, -3.1, step).tolist()\n",
    "x_trajectory += [-2.4] * len(y_steps)\n",
    "y_trajectory += y_steps\n",
    "\n",
    "# Final pause\n",
    "x_trajectory += [x_trajectory[-1]] * 20\n",
    "y_trajectory += [y_trajectory[-1]] * 20\n",
    "\n",
    "# Constant height (use average microphone height)\n",
    "z_trajectory = [np.mean(mic_positions[:, 1])] * len(y_trajectory)\n",
    "\n",
    "\n",
    "speaker_position = speaker_positions[0]\n",
    "speaker_rotation = poses[\"rot\"][0]\n",
    "\n",
    "# Visualize the trajectory in 2D top-down view\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(mic_positions[:, 2], mic_positions[:, 0], s=0.5, label='Training poses', alpha=0.5)\n",
    "plt.scatter(speaker_positions[:, 2], speaker_positions[:, 0], s=0.5, label='Speaker positions', alpha=0.5)\n",
    "plt.plot(y_trajectory, x_trajectory, 'r-', linewidth=2, label='Camera trajectory')\n",
    "plt.scatter(speaker_position[2], speaker_position[0], c='red', s=100, marker='*', label='Speaker location')\n",
    "plt.legend()\n",
    "plt.xlabel('Y position')\n",
    "plt.ylabel('X position')\n",
    "plt.title('Top-down view of room with camera trajectory')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trajectory array and save to file\n",
    "trajectory_data = np.array([x_trajectory, z_trajectory, y_trajectory]).T\n",
    "trajectory_dict = {\n",
    "    'rots': speaker_rotation,\n",
    "    'mic_poses': trajectory_data,\n",
    "    'source_poses': speaker_position\n",
    "}\n",
    "\n",
    "output_file = os.path.join(output_path_traj, \"test_trajectory_emptyroom.npy\")\n",
    "os.makedirs(output_path_traj, exist_ok=True)\n",
    "np.save(output_file, trajectory_dict)\n",
    "\n",
    "print(f\"Trajectory saved to: {output_file}\")\n",
    "print(f\"Trajectory length: {len(trajectory_data)} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6fb2ca",
   "metadata": {},
   "source": [
    "## Step 2: Generate Audio from NeRAF \n",
    "\n",
    "Use the NeRAF model to synthesize room impulse responses for each pose along the trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65bcb4",
   "metadata": {},
   "source": [
    "### Render Audio \n",
    "\n",
    "Use the `AVN_RENDER_POSES` environment variable to specify the trajectory for rendering. Execute the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "AVN_RENDER_POSES=/path/to/trajectory.npy \\\n",
    "  ns-eval \\\n",
    "  --load-config ../weights/RAF/EmptyRoom_NeRAF/NeRAF/2024-10-10_041356/config.yml \\\n",
    "  --render-output-path ./path/to/output/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d0b75",
   "metadata": {},
   "source": [
    "## Step 3: Generate Images from NeRAF \n",
    "\n",
    "Render images from the trained NeRAF model along the camera trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f9928",
   "metadata": {},
   "source": [
    "### Step 3a: Open Nerfstudio Viewer\n",
    "\n",
    "Open the nerfstudio viewer in your terminal:\n",
    "```bash\n",
    "ns-viewer --load-config ../weights/RAF/EmptyRoom_NeRAF/NeRAF/2024-10-10_041356/config.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd6d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scene contraction for coordinate space transformation\n",
    "scene_contraction = SceneContraction(order=float(\"inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cbcacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trajectory file that will be used for rendering\n",
    "trajectory_file = os.path.join(output_path_traj, \"test_trajectory_emptyroom.npy\")\n",
    "trajectory = np.load(trajectory_file, allow_pickle=True).item()\n",
    "\n",
    "print(f\"Loaded trajectory with {len(trajectory['mic_poses'])} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b2bfd",
   "metadata": {},
   "source": [
    "### Step 3b: Nerfstudio Trajectory Calibration \n",
    "\n",
    "The following step transforms the trajectory poses to align with Nerfsudio's viewer. \n",
    "\n",
    "⚠️ **Important:** The calibration parameters (angle corrections, translations, scaling factors) are specific to each scene and must be adjusted by visual inspection in the viewer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5630fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Transform trajectory poses to NeRFStudio coordinate system\n",
    "# ============================================================================\n",
    "\n",
    "extrinsics = []\n",
    "\n",
    "# CALIBRATION PARAMETERS - Adjust these for your room/model\n",
    "angle_correction = 14  # Rotation correction - specific to EmptyRoom\n",
    "translation_offset = np.array([1.6, -0.5, -1.4])  # Position offset - specific to EmptyRoom\n",
    "scale_factor = 2.3  # Scale factor for coordinate space - specific to EmptyRoom\n",
    "\n",
    "# Process each frame in the trajectory\n",
    "viewpoint_positions = trajectory['mic_poses']\n",
    "last_angle = 0\n",
    "\n",
    "for frame_idx, camera_pos in enumerate(viewpoint_positions):\n",
    "    # Compute camera orientation based on trajectory direction\n",
    "    next_frame_idx = min(frame_idx + 5, len(viewpoint_positions) - 1)\n",
    "    next_pos = viewpoint_positions[next_frame_idx]\n",
    "    \n",
    "    # If we're at a pause point (same position), keep previous angle\n",
    "    if next_pos[0] == camera_pos[0] and next_pos[2] == camera_pos[2]:\n",
    "        yaw_angle = last_angle\n",
    "    else:\n",
    "        # Compute angle pointing towards next position\n",
    "        yaw_angle = np.rad2deg(np.arctan2(next_pos[0] - camera_pos[0], \n",
    "                                          next_pos[2] - camera_pos[2]))\n",
    "    last_angle = yaw_angle\n",
    "    \n",
    "    # Create rotation (pitch=90°, roll=0°, yaw=computed)\n",
    "    euler_angles = np.array([90, 0, yaw_angle - 180 + angle_correction])\n",
    "    rotation_quat = R.from_euler('xyz', euler_angles, degrees=True).as_quat()\n",
    "    # Convert to (w, x, y, z)\n",
    "    rotation_quat = np.array([rotation_quat[3], rotation_quat[0], \n",
    "                              rotation_quat[1], rotation_quat[2]])\n",
    "    \n",
    "    # Transform position to viewer coordinates\n",
    "    position = np.array([camera_pos[0], -camera_pos[2], camera_pos[1]])\n",
    "    \n",
    "    # Apply initial rotation correction\n",
    "    angle_rot = R.from_euler('xyz', [0, 0, angle_correction], degrees=True)\n",
    "    position = angle_rot.apply(position)\n",
    "    \n",
    "    # Apply translation offset\n",
    "    position = position + translation_offset\n",
    "    \n",
    "    # Create Nerfstudio camera pose matrix\n",
    "    camera_pose = tf.SE3.from_rotation_and_translation(\n",
    "        tf.SO3(rotation_quat),\n",
    "        (position / VISER_NERFSTUDIO_SCALE_RATIO) * scale_factor,\n",
    "    )\n",
    "    \n",
    "    extrinsics.append(camera_pose.as_matrix())\n",
    "\n",
    "# Duplicate last frame \n",
    "extrinsics.append(extrinsics[-1])\n",
    "\n",
    "print(f\"Computed extrinsics for {len(extrinsics)} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a08b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example camera path template\n",
    "example_json_path = \"./example.json\"\n",
    "with open(example_json_path, 'r') as f:\n",
    "    camera_path_data = json.load(f)\n",
    "\n",
    "# Create the final camera path configuration\n",
    "camera_path_config = camera_path_data.copy()\n",
    "camera_path_config[\"default_fov\"] = 90\n",
    "camera_path_config[\"default_transition_sec\"] = 1\n",
    "camera_path_config[\"fps\"] = 20\n",
    "camera_path_config[\"seconds\"] = len(extrinsics) / camera_path_config[\"fps\"]\n",
    "camera_path_config[\"render_width\"] = 512\n",
    "camera_path_config[\"render_height\"] = 512\n",
    "camera_path_config[\"keyframes\"] = []\n",
    "camera_path_config[\"camera_path\"] = []\n",
    "\n",
    "# Create keyframe and camera path templates\n",
    "keyframe_template = {\n",
    "    'matrix': [],\n",
    "    'fov': 90.0,\n",
    "    'aspect': 1.0,\n",
    "    'override_transition_enabled': False,\n",
    "    'override_transition_sec': None\n",
    "}\n",
    "\n",
    "camera_path_template = {\n",
    "    'camera_to_world': [],\n",
    "    'fov': 90.0,\n",
    "    'aspect': 1.0\n",
    "}\n",
    "\n",
    "# Populate keyframes and camera path with extrinsics\n",
    "for extrinsic_matrix in extrinsics:\n",
    "    matrix_flat = extrinsic_matrix.flatten().tolist()\n",
    "    \n",
    "    # Add keyframe\n",
    "    keyframe = keyframe_template.copy()\n",
    "    keyframe['matrix'] = matrix_flat\n",
    "    camera_path_config[\"keyframes\"].append(keyframe)\n",
    "    \n",
    "    # Add camera path entry\n",
    "    cam_path = camera_path_template.copy()\n",
    "    cam_path['camera_to_world'] = matrix_flat\n",
    "    camera_path_config[\"camera_path\"].append(cam_path)\n",
    "\n",
    "# Save the corrected camera path\n",
    "save_path = os.path.join(room, \"camera_paths/test-sim-traj.json\")\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(camera_path_config, f, indent=2)\n",
    "\n",
    "print(f\"Camera path saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82359ef4",
   "metadata": {},
   "source": [
    "### Step 3c: Validate and Generate Final Rendering Command\n",
    "\n",
    "1. **Load and validate the trajectory in the viewer:**\n",
    "   - In the nerfstudio viewer, go to the \"Render\" panel\n",
    "   - Click \"Load path\" and select the generated camera path JSON file\n",
    "   - Verify the camera positions align correctly with the room geometry\n",
    "\n",
    "2. **Adjust calibration if needed:**\n",
    "   - If the path doesn't align, return to the previous cell and modify:\n",
    "     - `angle_correction`: Rotation offset in degrees\n",
    "     - `translation_offset`: Position offset (x, y, z)\n",
    "     - `scale_factor`: Overall coordinate scaling\n",
    "   - Re-run the trajectory generation and reload in the viewer\n",
    "\n",
    "3. **Configure rendering parameters in the viewer:**\n",
    "   - FOV: 90°\n",
    "   - Resolution: 512×512\n",
    "   - Spline tension: 0.5  \n",
    "   - FPS: 20\n",
    "   - Transition duration: 0.1 seconds (for RAF dataset)\n",
    "\n",
    "4. **Generate the rendering command:**\n",
    "   - In the viewer panel, click \"Generate command\" to interpolate keyframes\n",
    "   - The command will be displayed in the terminal\n",
    "   - Copy and modify it as needed:\n",
    "\n",
    "```bash\n",
    "ns-render camera-path \\\n",
    "  --load-config ./path/to/config.yml \\\n",
    "  --camera-path-filename ./path/to/camera-path.json \\\n",
    "  --output-format images \\\n",
    "  --output-path ./output_directory/\n",
    "```\n",
    "\n",
    "**Expected output:** Sequential frame files in the output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c1f02",
   "metadata": {},
   "source": [
    "## Step 4: Assemble Audio-Visual Video\n",
    "\n",
    "Combine the synthesized audio and rendered images to create the final video file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140e244",
   "metadata": {},
   "source": [
    "### Prepare for Video Assembly\n",
    "\n",
    "This section convoles the synthesized RIRs with an anechoic audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio processing configuration\n",
    "SAMPLE_RATE = 48000  # Hz\n",
    "STFT_N_FFT = 1024\n",
    "STFT_HOP_LENGTH = 256\n",
    "STFT_WIN_LENGTH = 512\n",
    "\n",
    "# Initialize Griffin-Lim algorithm for magnitude-to-waveform conversion (on GPU for speed)\n",
    "griffin_lim = GriffinLim(n_fft=STFT_N_FFT, hop_length=STFT_HOP_LENGTH, \n",
    "                         win_length=STFT_WIN_LENGTH, power=1).cuda()\n",
    "\n",
    "print(\"Audio processing configuration initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d05555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the anechoic audio source (dry audio without spatial effects)\n",
    "anechoic_audio_path = \"./DontMeanAthing_Mix.wav\"\n",
    "input_audio, sample_rate_audio = librosa.load(anechoic_audio_path, sr=SAMPLE_RATE, mono=True)\n",
    "\n",
    "# Locate all generated images and audio features (STFTs) from the rendering\n",
    "rendered_images = sorted(glob.glob(os.path.join(output_path_video, '*.jpg')))\n",
    "generated_stfts = sorted(glob.glob(os.path.join(output_path_video, 'eval_*.npy')))\n",
    "\n",
    "print(f\"Found {len(rendered_images)} images and {len(generated_stfts)} STFT files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9aea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronize audio with video frames\n",
    "fps = 10  # Frames per second of generated audio/images\n",
    "time_between_frames = 1 / fps  # Duration of each frame in seconds\n",
    "\n",
    "# Create timestamps for each frame\n",
    "frame_timestamps = np.arange(0, len(generated_stfts) * time_between_frames, time_between_frames)\n",
    "# Add one more timestamp for the final frame\n",
    "frame_timestamps = np.append(frame_timestamps, frame_timestamps[-1] + time_between_frames)\n",
    "\n",
    "# Prepare audio to match the total duration\n",
    "total_duration_seconds = frame_timestamps[-1]\n",
    "total_samples_needed = int(total_duration_seconds * SAMPLE_RATE)\n",
    "\n",
    "# Tile the input audio to match required duration\n",
    "input_audio = np.tile(input_audio, int(np.ceil(total_samples_needed / len(input_audio))))\n",
    "input_audio = input_audio[:total_samples_needed]\n",
    "\n",
    "print(f\"Synchronized audio: {total_duration_seconds:.2f}s ({total_samples_needed} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa142799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Generate audio by convolving RIRs with anechoic signal\n",
    "# ============================================================================\n",
    "\n",
    "max_output_samples = int(frame_timestamps[-1] * SAMPLE_RATE)\n",
    "output_audio = np.zeros((2, max_output_samples))  # Stereo output\n",
    "\n",
    "# Setup overlap-add windowing for smooth transitions between frames\n",
    "frame_length = int(time_between_frames * SAMPLE_RATE)\n",
    "window_overlap = frame_length // 2  # 50% overlap between frames\n",
    "hann_window = scipy.signal.windows.hann(int(frame_length + window_overlap * 2), sym=False)\n",
    "hann_normalization = np.zeros((1, max_output_samples))\n",
    "\n",
    "print(f\"Processing {len(generated_stfts)} audio frames...\")\n",
    "\n",
    "for frame_idx in tqdm.tqdm(range(len(generated_stfts))):\n",
    "    # Load the STFT magnitude spectrum for this frame\n",
    "    stft_magnitude = np.load(generated_stfts[frame_idx])\n",
    "    \n",
    "    # Compute time position for this frame with overlap-add\n",
    "    time_position = int(frame_idx * frame_length - window_overlap)\n",
    "    \n",
    "    # Special handling for first and last frames\n",
    "    if frame_idx == 0:\n",
    "        time_position = 0\n",
    "        window = hann_window[window_overlap:].copy()\n",
    "        window[:frame_length // 2] = 1.0  # Ramp up\n",
    "    elif frame_idx == len(generated_stfts) - 1:\n",
    "        window = hann_window[:-window_overlap].copy()\n",
    "        window[-frame_length // 2:] = 1.0  # Ramp down\n",
    "    else:\n",
    "        window = hann_window\n",
    "    \n",
    "    # Accumulate window values for normalization\n",
    "    hann_normalization[0, time_position:time_position + len(window)] += window\n",
    "    \n",
    "    # Convert STFT magnitude back to time-domain waveform using Griffin-Lim\n",
    "    magnitude_clipped = np.clip(np.exp(stft_magnitude) - 1e-3, 0.0, 10000.0)\n",
    "    rir_waveform = griffin_lim(torch.from_numpy(magnitude_clipped).cuda()).cpu().numpy()\n",
    "    \n",
    "    # Convolve anechoic signal with the RIR\n",
    "    convolved_0 = fftconvolve(input_audio, rir_waveform[0, :])\n",
    "    # RAF dataset is mono, so duplicate for stereo (this is different in SoundSpaces)\n",
    "    output_audio_frame = np.vstack((convolved_0, convolved_0))\n",
    "    \n",
    "    # Apply windowing and overlap-add\n",
    "    output_audio_frame = output_audio_frame.astype(np.float32)\n",
    "    output_audio_frame = output_audio_frame[:, time_position:time_position + len(window)]\n",
    "    output_audio_frame = output_audio_frame * window\n",
    "    \n",
    "    output_audio[:, time_position:time_position + len(window)] += output_audio_frame\n",
    "\n",
    "# Normalize\n",
    "output_audio = output_audio / np.max(np.abs(output_audio)) \n",
    "output_audio = output_audio.astype(np.float32)\n",
    "\n",
    "# Save the final spatialized audio\n",
    "audio_output_path = os.path.join(output_path_video, \"audio.wav\")\n",
    "wavfile.write(audio_output_path, SAMPLE_RATE, output_audio.T)\n",
    "print(f\"Audio saved to: {audio_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine rendered images and spatialized audio into final video\n",
    "video_framerate = 20  # fps for final video\n",
    "output_video_path = os.path.join(output_path_video, \"video.mp4\")\n",
    "\n",
    "ffmpeg_command = (\n",
    "    f\"ffmpeg -r {video_framerate} -i {output_path_video}/%05d.jpg \"\n",
    "    f\"-i {audio_output_path} \"\n",
    "    f\"-c:v libx264 -c:a aac -pix_fmt yuv420p -shortest \"\n",
    "    f\"{output_video_path} -y\"\n",
    ")\n",
    "\n",
    "print(\"Executing FFmpeg to create video...\")\n",
    "os.system(ffmpeg_command)\n",
    "print(f\"✓ Video saved to: {output_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44f0ab",
   "metadata": {},
   "source": [
    "## Step 5: Generate Trajectory Visualization Mini-Maps (Optional)\n",
    "\n",
    "Create a video showing the camera trajectory overlaid on a top-down view of the room for visualization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43cbcca",
   "metadata": {},
   "source": [
    "This step is optional but useful for visualization. It generates a synchronized mini-map showing the camera path over a top-down view of the scene, which can be inserted as a corner graphic in the final video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b24142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Prepare top-down view for trajectory visualization\n",
    "# ============================================================================\n",
    "\n",
    "# Load background image (top-down view of the room)\n",
    "topview_image_path = \"./top_view_empty.png\"\n",
    "topview_image = plt.imread(topview_image_path)\n",
    "\n",
    "# Define mapping between image coordinates and room coordinates\n",
    "# These values must be determined by visual inspection of the image\n",
    "image_bounds = {\n",
    "    'x_min': 300,    # left edge\n",
    "    'x_max': 1510,   # right edge\n",
    "    'y_min': 200,    # top edge\n",
    "    'y_max': 970,    # bottom edge\n",
    "}\n",
    "\n",
    "# Corresponding room coordinate bounds\n",
    "room_bounds_x = [aabb[0][2], aabb[1][2]]  # X range from point cloud\n",
    "room_bounds_y = [aabb[0][0], aabb[1][0]]  # Y range from point cloud\n",
    "\n",
    "# Compute scaling factors to map room coordinates to image coordinates\n",
    "scale_x = ((image_bounds['x_max'] - image_bounds['x_min']) / \n",
    "           (room_bounds_x[1] - room_bounds_x[0])) - 43\n",
    "scale_y = ((image_bounds['y_max'] - image_bounds['y_min']) / \n",
    "           (room_bounds_y[1] - room_bounds_y[0])) - 30\n",
    "\n",
    "# Transform trajectory points to image coordinates\n",
    "trajectory_x_image = ((x_trajectory - room_bounds_y[0]) * scale_y + \n",
    "                      image_bounds['y_min']) + 100\n",
    "trajectory_y_image = ((y_trajectory - room_bounds_x[0]) * scale_x + \n",
    "                      image_bounds['x_min']) + 200\n",
    "\n",
    "# Transform speaker position to image coordinates\n",
    "speaker_x_image = ((speaker_position[0] - room_bounds_y[0]) * scale_y + \n",
    "                   image_bounds['y_min']) + 100\n",
    "speaker_y_image = ((speaker_position[2] - room_bounds_x[0]) * scale_x + \n",
    "                   image_bounds['x_min']) + 200\n",
    "\n",
    "# Preview the trajectory on the map\n",
    "plt.figure()\n",
    "plt.imshow(topview_image)\n",
    "plt.plot(trajectory_y_image, -trajectory_x_image + 1150, 'r-')\n",
    "plt.scatter(speaker_y_image, -speaker_x_image + 1150, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e586006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Generate mini-map video frames\n",
    "# ============================================================================\n",
    "\n",
    "minimap_output_dir = os.path.join(output_path_video, \"maps_ER_video\")\n",
    "os.makedirs(minimap_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Generating {len(trajectory_x_image)} minimap frames...\")\n",
    "\n",
    "for frame_index in tqdm.tqdm(range(len(trajectory_x_image))):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Display background image\n",
    "    ax.imshow(topview_image)\n",
    "    \n",
    "    # Plot trajectory up to current frame\n",
    "    ax.scatter(trajectory_y_image[:frame_index + 1], \n",
    "              -trajectory_x_image[:frame_index + 1] + 1150,\n",
    "              c='green', label='Trajectory', s=20)\n",
    "    \n",
    "    # Plot speaker location\n",
    "    ax.scatter(speaker_y_image, -speaker_x_image + 1150, \n",
    "              c='blue', label='Source', s=20, marker='x')\n",
    "    \n",
    "    # Configure display\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_xlim([image_bounds['x_min'], image_bounds['x_max']])\n",
    "    ax.set_ylim([image_bounds['y_max'], image_bounds['y_min']])\n",
    "    ax.legend(fontsize=10)\n",
    "    \n",
    "    # Save frame\n",
    "    frame_path = os.path.join(minimap_output_dir, f\"map_{frame_index:05d}.png\")\n",
    "    plt.savefig(frame_path, transparent=False)\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"Generated {len(trajectory_x_image)} minimap frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mini-map video from generated frames\n",
    "minimap_video_path = os.path.join(minimap_output_dir, \"minimaps.mp4\")\n",
    "minimap_fps = 10  # frames per second\n",
    "\n",
    "ffmpeg_minimap_command = (\n",
    "    f\"ffmpeg -r {minimap_fps} -i {minimap_output_dir}/map_%05d.png \"\n",
    "    f\"-c:v libx264 -pix_fmt yuv420p \"\n",
    "    f\"{minimap_video_path} -y\"\n",
    ")\n",
    "\n",
    "print(\"Creating mini-map video...\")\n",
    "os.system(ffmpeg_minimap_command)\n",
    "print(f\"✓ Mini-map video saved to: {minimap_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3eedcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
