{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e7c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dependencies for loudness map visualization\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from librosa.feature import rms\n",
    "from scipy.spatial.transform import Rotation as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb9713c",
   "metadata": {},
   "source": [
    "# NeRAF Loudness Maps Visualization\n",
    "\n",
    "This notebook illustrates how to generate and visualize acoustic loudness maps using NeRAF. The workflow includes:\n",
    "1. Creating a dense grid of microphone poses within a scene\n",
    "2. Rendering RIRs using NeRAF\n",
    "3. Computing loudness values across the grid\n",
    "4. Visualizing the loudness distribution as a heatmap\n",
    "\n",
    "**Prerequisites:** A trained NeRAF model and nerfstudio installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1300724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update these paths to match your setup\n",
    "path_data = './NeRAF_loudness_maps'  # Output directory for rendered audio\n",
    "path_dataset = '/path/to/raf_dataset'  # Dataset root\n",
    "room = os.path.join(path_dataset, \"FurnishedRoom\")  # Specific room dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc9f9b7",
   "metadata": {},
   "source": [
    "## Step 1: Create Dense Grid of Microphone Poses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5af937",
   "metadata": {},
   "source": [
    "This section creates a dense grid of microphone poses at multiple heights and a fixed source pose, then prepares this data for NeRAF rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_poses(files, room):\n",
    "    \"\"\"\n",
    "    Process microphone and source positions from dataset files.\n",
    "    \n",
    "    Args:\n",
    "        files: List of file identifiers to process\n",
    "        room: Root directory containing the room data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with keys: 'rot', 'mic_pose', 'source_pose', 'rot_degree'\n",
    "    \"\"\"\n",
    "    poses_list = []\n",
    "    \n",
    "    for f in files:\n",
    "        rx_file = os.path.join(room, 'data', f, 'rx_pos.txt')  # Receiver (microphone) file\n",
    "        tx_file = os.path.join(room, 'data', f, 'tx_pos.txt')  # Transmitter (source) file\n",
    "        \n",
    "        # Load receiver positions\n",
    "        with open(rx_file, 'r') as file:\n",
    "            rx = file.readlines()\n",
    "            rx = [line.replace('\\n', '').split(',') for line in rx]\n",
    "            rx = [[float(j) for j in i] for i in rx]\n",
    "            rx_pose = np.array(rx[0])\n",
    "        \n",
    "        # Load transmitter: quaternion (xyzW) and position\n",
    "        with open(tx_file, 'r') as file:\n",
    "            tx = file.readlines()\n",
    "            tx = [line.replace('\\n', '').split(',') for line in tx]\n",
    "            tx = [[float(j) for j in i] for i in tx]\n",
    "            tx = np.array(tx[0])\n",
    "        \n",
    "        # Extract quaternion and pose\n",
    "        quat = tx[:4]  # Quaternion format: xyzW\n",
    "        tx_pose = tx[4:]  # Source position\n",
    "        \n",
    "        # Convert quaternion to rotation using Euler angles (YXZ convention)\n",
    "        r = T.from_quat(quat)\n",
    "        spk_rot_euler = r.as_euler('yxz', degrees=True)\n",
    "        spk_rot_deg = np.round(spk_rot_euler[0], decimals=0)  # Y-axis rotation\n",
    "        \n",
    "        # Convert rotation degree to a normalized vector for SHE (Spherical Harmonics)\n",
    "        rad_spk_rot = np.deg2rad(spk_rot_deg)\n",
    "        spk_rot_vec = np.array([np.cos(rad_spk_rot), 0, np.sin(rad_spk_rot)])\n",
    "        spk_rot_norm = (spk_rot_vec + 1.0) / 2.0  # Normalize to [0, 1]\n",
    "        \n",
    "        # Store pose information\n",
    "        poses_list.append({\n",
    "            'mic_pose': np.expand_dims(rx_pose, axis=0),\n",
    "            'source_pose': np.expand_dims(tx_pose, axis=0),\n",
    "            'rot': np.expand_dims(spk_rot_norm, axis=0),\n",
    "            'rot_degree': np.expand_dims(np.array([spk_rot_deg], dtype=float), axis=0)\n",
    "        })\n",
    "    \n",
    "    # Concatenate all poses\n",
    "    result = {\n",
    "        'mic_pose': np.concatenate([p['mic_pose'] for p in poses_list], axis=0),\n",
    "        'source_pose': np.concatenate([p['source_pose'] for p in poses_list], axis=0),\n",
    "        'rot': np.concatenate([p['rot'] for p in poses_list], axis=0),\n",
    "        'rot_degree': np.concatenate([p['rot_degree'] for p in poses_list], axis=0)\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e86120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset split information (which scenes are in train/test)\n",
    "split_file = os.path.join(room, 'metadata/data-split.json')\n",
    "with open(split_file) as f:\n",
    "    split_data = json.load(f)\n",
    "\n",
    "split_files_train = split_data['train'][0]\n",
    "split_files_test = split_data['test'][0]\n",
    "split_files_all = split_files_train + split_files_test\n",
    "\n",
    "print(f\"Train files: {len(split_files_train)}, Test files: {len(split_files_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a8bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process all poses and compute scene boundaries\n",
    "poses = _process_poses(split_files_all, room)\n",
    "\n",
    "# Calculate AABB (Axis-Aligned Bounding Box) of scene\n",
    "aabb = np.array([poses['mic_pose'].min(axis=0), poses['mic_pose'].max(axis=0)])\n",
    "print(f\"Scene AABB (min): {aabb[0]}\")\n",
    "print(f\"Scene AABB (max): {aabb[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract microphone and source poses \n",
    "p_mic = poses[\"mic_pose\"]  # Shape: (N, 3) - all microphone positions\n",
    "p_source = poses[\"source_pose\"]  # Shape: (N, 3) - all source positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274bee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top-down view of scene with all microphone (blue) and source (orange) positions\n",
    "# This helps in selecting an appropriate source location for rendering\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(p_mic[:, 2], p_mic[:, 0], s=1, alpha=0.5, label='Microphones')\n",
    "plt.scatter(p_source[:, 2], p_source[:, 0], s=20, alpha=0.7, label='Sources')\n",
    "plt.xlabel('Z (m)')\n",
    "plt.ylabel('X (m)')\n",
    "plt.title('Scene Layout (Top-Down View)')\n",
    "plt.legend()\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13851506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dense regular grid of microphone poses for rendering\n",
    "step = 0.1  # Density\n",
    "\n",
    "# Add margin around the AABB to ensure full scene coverage\n",
    "# Note: Positions outside the scene will be ignored in later visualization\n",
    "margin = 0.5\n",
    "min_x = aabb[0][0] - margin\n",
    "max_x = aabb[1][0] + margin\n",
    "min_y = aabb[0][2] - margin\n",
    "max_y = aabb[1][2] + margin\n",
    "\n",
    "# Extract unique heights from the original microphone positions\n",
    "microphone_heights = np.unique(np.round(p_mic[:, 1], decimals=1))\n",
    "\n",
    "# Create 3D grid using meshgrid\n",
    "x, y, z = np.meshgrid(\n",
    "    np.arange(min_x, max_x, step),  # X positions\n",
    "    np.arange(min_y, max_y, step),  # Y positions (horizontal distance)\n",
    "    microphone_heights  # Z positions (heights)\n",
    ")\n",
    "\n",
    "# Flatten to create list of poses\n",
    "x_flat = x.flatten()\n",
    "y_flat = y.flatten()\n",
    "z_flat = z.flatten()\n",
    "\n",
    "print(f\"Generated {len(x_flat)} microphone poses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a source position and rotation for rendering\n",
    "idx_source = 0  # Which source to use\n",
    "source_pose_selected = p_source[idx_source]\n",
    "\n",
    "# Select source rotation (azimuth angle in degrees)\n",
    "# This controls the direction the speaker is \"facing\"\n",
    "rotation_deg = 65\n",
    "\n",
    "# Convert rotation to normalized rotation vector for NeRAF\n",
    "rad_rotation = np.deg2rad(rotation_deg)\n",
    "rot_vector = np.array([np.cos(rad_rotation), 0, np.sin(rad_rotation)])\n",
    "source_rotation_normalized = (rot_vector + 1.0) / 2.0  # Normalize to [0, 1]\n",
    "\n",
    "print(f\"Source position: {source_pose_selected}\")\n",
    "print(f\"Source rotation: {rotation_deg}Â°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dense grid overlaid on the original scene\n",
    "plt.figure(figsize=(10, 10), dpi=500)\n",
    "plt.scatter(p_mic[:, 2], p_mic[:, 0], s=0.5)\n",
    "plt.scatter(p_source[:, 2], p_source[:, 0], s=0.5)\n",
    "plt.plot(y_flat, x_flat, 'x')\n",
    "plt.scatter(source_pose_selected[2], source_pose_selected[0], c='r')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "#plt.savefig('layout.png',dpi=500, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf90049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format poses for NeRAF rendering\n",
    "# NeRAF expects poses in (X, Z, Y) order\n",
    "pos_grid_for_neraf = np.array([x_flat, z_flat, y_flat]).T\n",
    "\n",
    "# Create dictionary in NeRAF format\n",
    "poses_for_rendering = {\n",
    "    'rots': source_rotation_normalized,  # Source rotation\n",
    "    'mic_poses': pos_grid_for_neraf,     # Microphone positions\n",
    "    'source_poses': source_pose_selected  # Source position\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dde06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save poses for NeRAF rendering\n",
    "output_filename = f\"loudness_S{idx_source}_Rot{rotation_deg}.npy\"\n",
    "output_path = os.path.join(path_data, output_filename)\n",
    "np.save(output_path, poses_for_rendering)\n",
    "print(f\"Saved rendering poses to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26015053",
   "metadata": {},
   "source": [
    "## Step 2: Render and Compute Loudness Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1844d46d",
   "metadata": {},
   "source": [
    "### Rendering with NeRAF\n",
    "\n",
    "Before running the following cells, you must generate Room Impulse Responses (RIRs) using NeRAF:\n",
    "\n",
    "```bash\n",
    "AVN_RENDER_POSES=your_poses.npy ns-eval \\\n",
    "  --load-config path/to/config.yml \\\n",
    "  --render-output-path ./output_folder\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- `AVN_RENDER_POSES`: Path to the .npy file with poses (created in Step 1)\n",
    "- `config.yml`: NeRAF model configuration file\n",
    "- `output_folder`: Where rendered audio files will be saved\n",
    "\n",
    "The command will generate RIR audio STFT (in .npy format) for each microphone pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edea71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rendered audio and corresponding poses\n",
    "# Update these paths to match your rendering output\n",
    "audio_data_dir = os.path.join(path_data, f'loudness_S{idx_source}_Rot{rotation_deg}')  # Directory with rendered RIRs\n",
    "poses_file = os.path.join(path_data, f'loudness_S{idx_source}_Rot{rotation_deg}.npy')  # Poses file from Step 1\n",
    "\n",
    "poses_loc = np.load(poses_file, allow_pickle=True).item()\n",
    "print(f\"Loaded poses from: {poses_file}\")\n",
    "print(f\"Audio directory: {audio_data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and prepare poses for analysis\n",
    "mic_poses = poses_loc['mic_poses']  # Shape: (N, 3)\n",
    "source_poses = poses_loc['source_poses']  # Single source position\n",
    "rots = poses_loc['rots']  # Single source rotation\n",
    "\n",
    "print(f\"Loaded {mic_poses.shape[0]} microphone poses\")\n",
    "print(f\"Source position shape: {source_poses.shape}\")\n",
    "\n",
    "# Replicate source position and rotation for each microphone pose\n",
    "# (one fixed source, multiple microphone positions)\n",
    "rots_expanded = np.expand_dims(rots, axis=0)\n",
    "rots_expanded = np.repeat(rots_expanded, mic_poses.shape[0], axis=0)\n",
    "\n",
    "source_poses_expanded = np.expand_dims(source_poses, axis=0)\n",
    "source_poses_expanded = np.repeat(source_poses_expanded, mic_poses.shape[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a70dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and sort all rendered RIR audio files\n",
    "audio_files = sorted([f for f in os.listdir(audio_data_dir) if f.endswith('.npy')])\n",
    "print(f\"Found {len(audio_files)} audio files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed75520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loudness (RMS) for each audio file\n",
    "# This can be time-consuming depending on grid density\n",
    "loudness_values = []\n",
    "\n",
    "for i, audio_file in enumerate(audio_files):\n",
    "    if (i + 1) % max(1, len(audio_files) // 10) == 0:\n",
    "        print(f\"Processing {i+1}/{len(audio_files)}...\")\n",
    "    \n",
    "    audio_data = np.load(os.path.join(audio_data_dir, audio_file))\n",
    "    # Clip to convert log-domain to linear domain\n",
    "    audio_linear = np.clip(np.exp(audio_data) - 1e-3, 0.0, 10000.0)\n",
    "    # Compute RMS (loudness) over the entire signal\n",
    "    loudness = rms(S=audio_linear[0], frame_length=audio_linear[0].shape[-2] * 2 - 2)\n",
    "    loudness_values.append(loudness)\n",
    "\n",
    "loudness_values = np.array(loudness_values)\n",
    "print(f\"Computed loudness for {len(loudness_values)} positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate loudness values by (X, Z) position at different heights\n",
    "# Filter to keep only microphones within a reasonable height range\n",
    "height_min = 0.0  # Minimum height in meters\n",
    "height_max = 2.0  # Maximum height in meters\n",
    "\n",
    "position_dict = {}\n",
    "\n",
    "for i in range(mic_poses.shape[0]):\n",
    "    loudness_val = loudness_values[i]\n",
    "    x_pos = mic_poses[i][0]\n",
    "    z_pos = mic_poses[i][2]\n",
    "    height = mic_poses[i][1]\n",
    "    \n",
    "    # Skip if outside height range\n",
    "    if height < height_min or height > height_max:\n",
    "        continue\n",
    "    \n",
    "    # Use (X, Z) as key for map\n",
    "    key = (x_pos, z_pos)\n",
    "    if key in position_dict:\n",
    "        position_dict[key].append(loudness_val)\n",
    "    else:\n",
    "        position_dict[key] = [loudness_val]\n",
    "\n",
    "print(f\"Aggregated to {len(position_dict)} unique (X, Z) positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aa3bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates and corresponding loudness values\n",
    "x_coords = []\n",
    "z_coords = []\n",
    "loudness_lists = []\n",
    "\n",
    "for (x, z) in sorted(position_dict.keys()):\n",
    "    x_coords.append(x)\n",
    "    z_coords.append(z)\n",
    "    loudness_lists.append(position_dict[(x, z)])\n",
    "\n",
    "x_coords = np.array(x_coords)\n",
    "z_coords = np.array(z_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5887cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loudness map as a heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 10), dpi=500)\n",
    "\n",
    "# Create scatter plot with color representing loudness\n",
    "scatter = ax.scatter(z_coords, x_coords, c=np.log(np.sum(loudness_lists, axis=(1, 2, 3))), s=9, marker='s', \n",
    "                    cmap='coolwarm')\n",
    "\n",
    "# Mark source position\n",
    "ax.scatter(source_poses_expanded[0, 2], source_poses_expanded[0, 0], c='red')\n",
    "ax.set_aspect('equal', 'box')\n",
    "\n",
    "cbar = fig.colorbar(scatter, ax=ax)\n",
    "# plt.savefig(f'path/to/loudness_S{idx_source}_Rot{rotation_deg}.png', dpi=500, transparent=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e3e4a7",
   "metadata": {},
   "source": [
    "## Step 3: Create Final Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13e8e8e",
   "metadata": {},
   "source": [
    "### Further Processing\n",
    "\n",
    "To create a final publication-ready figure:\n",
    "\n",
    "1. **Obtain scene geometry:** Get a top-down view/mesh from the dataset\n",
    "2. **Overlay:** Save the loudness heatmap and overlay it on the scene geometry\n",
    "3. **Crop:** Remove areas outside the scene bounds for a cleaner visualization\n",
    "4. **Annotate:** Add source location marker (red dot) and other annotations as needed\n",
    "\n",
    "To ease the superposition of the loudness map on the top view, refer back to the \"Scene Layout\" visualization from Step 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
