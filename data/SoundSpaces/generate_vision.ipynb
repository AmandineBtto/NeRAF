{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json \n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import habitat_sim.sim\n",
    "from habitat_sim.utils.common import quat_from_coeffs\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import magnum as mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to generate RGB images from Habitat Sim necessary for NeRAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the simulator if it is already running to avoid crash\n",
    "try:  \n",
    "    sim.close()\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice of scene, dataset and path\n",
    "dataset = 'Replica'\n",
    "scene = 'office_4'\n",
    "scene_path = './sound-spaces/data/scene_datasets/replica' # update the path with yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulator Parameters\n",
    "scene_id = scene\n",
    "sim_settings = {\n",
    "    \"path\": scene_path,\n",
    "    'scene_id': scene_id,\n",
    "    \"scene\": os.path.join(scene_path, scene_id, 'habitat', 'mesh_semantic.ply'),  # Scene path\n",
    "    \"scene_dataset\": os.path.join(scene_path, scene_id, 'habitat', 'replica_stage.stage_config.json'),\n",
    "    'navmesh': os.path.join(scene_path, scene_id, 'habitat', 'mesh_semantic.navmesh'),\n",
    "    \"default_agent\": 0,  # Index of the default agent\n",
    "    \"sensor_height\": 1.5,  # Height of sensors in meters, relative to the agent\n",
    "    \"width\": 512,  # Spatial resolution of the observations\n",
    "    \"height\": 512,\n",
    "    \"hfov\": 90,\n",
    "    \"vfov\": 90,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_backend_config(settings, sensor_specs):\n",
    "    # simulator backend\n",
    "    backend_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    backend_cfg.gpu_device_id = 0\n",
    "    backend_cfg.scene_id = settings[\"scene\"]\n",
    "    backend_cfg.scene_dataset_config_file = settings[\"scene_dataset\"]\n",
    "    backend_cfg.load_semantic_mesh = True\n",
    "    backend_cfg.enable_physics = False\n",
    "\n",
    "    # agent\n",
    "    agent_cfg = habitat_sim.AgentConfiguration()\n",
    "    agent_cfg.sensor_specifications = sensor_specs\n",
    "\n",
    "    return  habitat_sim.Configuration(backend_cfg, [agent_cfg])\n",
    "\n",
    "def make_sensors(settings):\n",
    "    sensor_specs = []\n",
    "    # rgb sensor\n",
    "    rgb_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    rgb_sensor_spec.uuid = \"color_sensor\"\n",
    "    rgb_sensor_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "    rgb_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE #EQUIRECTANGULAR#PINHOLE\n",
    "    rgb_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    rgb_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    rgb_sensor_spec.orientation = [0.0, 0.0, 0.0]\n",
    "    rgb_sensor_spec.hfov = mn.Deg(settings[\"hfov\"])\n",
    "    sensor_specs.append(rgb_sensor_spec)\n",
    "\n",
    "    # registrer camera parameters\n",
    "    aspect_ratio = settings[\"width\"] / settings[\"height\"] \n",
    "    fx = (1 / np.tan(settings[\"hfov\"] * (np.pi/180)/ 2.))\n",
    "    fy = (1 / np.tan(settings[\"hfov\"] * (np.pi/180) / 2.)) * aspect_ratio # habitatdoc if width different height\n",
    "    fx_px = settings[\"width\"] / (2 * fx)\n",
    "    fy_px = settings[\"height\"] / (2 * fy)\n",
    "    settings['near'] = rgb_sensor_spec.near\n",
    "    settings['far'] = rgb_sensor_spec.far\n",
    "    settings['aspect ratio'] = aspect_ratio\n",
    "    settings['fx'] = fx\n",
    "    settings['fy'] = fy\n",
    "    settings['fx px'] =  fx_px\n",
    "    settings['fy px'] = fy_px\n",
    "\n",
    "\n",
    "    return sensor_specs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the simulator\n",
    "sensor_specs = make_sensors(sim_settings)\n",
    "cfg = make_backend_config(sim_settings, sensor_specs)\n",
    "sim = habitat_sim.Simulator(cfg)\n",
    "sim.pathfinder.load_nav_mesh(os.path.join(scene_path, scene_id, 'habitat', 'mesh_semantic.navmesh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open pkl files containing eval and train poses\n",
    "path2trainposes = os.path.join(scene_id, scene_id + '_Train.pkl')\n",
    "path2evalposes = os.path.join(scene_id, scene_id + '_Eval.pkl')\n",
    "\n",
    "with open(path2trainposes, 'rb') as f:\n",
    "    train_poses = pickle.load(f)\n",
    "with open(path2evalposes, 'rb') as f:\n",
    "    eval_poses = pickle.load(f)\n",
    "\n",
    "print('Number of train poses:', len(train_poses))\n",
    "print('Number of eval poses:', len(eval_poses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the observations \n",
    "\n",
    "display = False \n",
    "\n",
    "for set_data in ['train', 'eval']:\n",
    "    print('Generating:', set_data)\n",
    "    if set_data == 'train':\n",
    "        poses = train_poses\n",
    "        train_obs = {}\n",
    "    else:\n",
    "        poses = eval_poses\n",
    "        eval_obs = {}\n",
    "\n",
    "    num_obs = 0\n",
    "    color_sensor = sim.get_agent(0)._sensors[\"color_sensor\"]\n",
    "\n",
    "    for pt_idx in poses.keys():\n",
    "        data = poses[pt_idx]\n",
    "        angle = pt_idx[1]\n",
    "        pose = data['Position']\n",
    "        quat = quat_from_coeffs(data['Quaternion'])\n",
    "        \n",
    "        agent = sim.get_agent(0)\n",
    "        new_state = sim.get_agent(0).get_state()\n",
    "        new_state.position = np.array(pose) \n",
    "        new_state.rotation = quat\n",
    "        new_state.sensor_states = {}\n",
    "        agent.set_state(new_state, True)\n",
    "\n",
    "        obs = sim.get_sensor_observations()\n",
    "        obs['Quaternion'] = data['Quaternion'] # easier to also store them here for later use\n",
    "        obs['Position'] = data['Position']\n",
    "        \n",
    "        if display: \n",
    "            rgb = np.array(obs[\"color_sensor\"])\n",
    "            plt.imshow(rgb)\n",
    "            plt.show()\n",
    "            \n",
    "        num_obs += 1\n",
    "\n",
    "        if set_data == 'eval':\n",
    "            eval_obs[(tuple(pose), angle)] = obs\n",
    "        else:\n",
    "            train_obs[(tuple(pose), angle)] = obs\n",
    "            \n",
    "    print('Number of observations in', set_data, num_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go to nerfstudio format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create transforms.json in nerfstudio format and save the images\n",
    "# Load sim settings \n",
    "path2settings = os.path.join(scene_id, scene_id + '_SimParams.json')\n",
    "json_data = json.load(open(path2settings, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera intrinsics \n",
    "w = json_data[\"width\"]\n",
    "h = json_data[\"height\"]\n",
    "fl_x = json_data[\"fx px\"]\n",
    "fl_y = json_data[\"fy px\"]\n",
    "cx = w/2\n",
    "cy = h/2\n",
    "\n",
    "dict_transforms = {\n",
    "    \"camera_model\": \"OPENCV\",\n",
    "    \"orientation_override\": \"none\",\n",
    "    \"frames\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory that will contain the generated images\n",
    "i = 1\n",
    "imdir = os.path.join(scene_id, 'images')\n",
    "\n",
    "#remove the directory if it exists\n",
    "if os.path.exists(imdir):\n",
    "    for f in os.listdir(imdir):\n",
    "        os.remove(os.path.join(imdir, f))\n",
    "    os.rmdir(imdir)\n",
    "\n",
    "os.mkdir(imdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the images and the transforms.json\n",
    "\n",
    "for set_data in ['train', 'eval']: \n",
    "    if set_data == 'train':\n",
    "        obs = train_obs\n",
    "    else:\n",
    "        obs = eval_obs\n",
    "\n",
    "    for k, v in obs.items():\n",
    "        im_name = set_data + '_' + \"frame_{:05d}.jpg\".format(i)\n",
    "        pos = v[\"Position\"]\n",
    "        rot = v[\"Quaternion\"]\n",
    "\n",
    "        matrix_rot = R.from_quat(rot).as_matrix()\n",
    "        matrix_transform = np.eye(4)\n",
    "        matrix_transform[:3, :3] = matrix_rot\n",
    "        matrix_transform[:3, 3] = pos\n",
    "        matrix_transform[3, 3] = 1\n",
    "\n",
    "        # camera to world from right up back (camera coordinates) to left up back (world coordinates)\n",
    "        matrix_transform = np.array([\n",
    "            [-1, 0, 0, 0],\n",
    "            [0, 0, 1, 0],\n",
    "            [0, 1, 0, 0],\n",
    "            [0, 0, 0, 1]\n",
    "        ]).dot(matrix_transform)\n",
    "\n",
    "\n",
    "        im = v[\"color_sensor\"]\n",
    "        frame = {\n",
    "            \"fl_x\": fl_x,\n",
    "            \"fl_y\": fl_y,\n",
    "            \"cx\": cx,\n",
    "            \"cy\": cy,\n",
    "            \"w\": w,\n",
    "            \"h\": h,\n",
    "            \"file_path\": os.path.join(imdir, im_name),\n",
    "            \"transform_matrix\": matrix_transform.tolist()\n",
    "        }\n",
    "\n",
    "        dict_transforms[\"frames\"].append(frame)\n",
    "        plt.imsave(os.path.join(imdir, im_name), im)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove transforms.json if exists and save the new one\n",
    "if os.path.exists(os.path.join(scene_id, \"transforms.json\")):\n",
    "    os.remove(os.path.join(scene_id, \"transforms.json\"))\n",
    "with open(os.path.join(scene_id, \"transforms.json\"), \"w\") as f:\n",
    "    json.dump(dict_transforms, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
