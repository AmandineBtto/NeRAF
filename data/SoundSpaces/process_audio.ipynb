{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Spectrogram\n",
    "from scipy.io import wavfile\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to process audio into STFT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pre-process the binaural RIRs from SoundSpaces 1.0 for faster training. \n",
    "We subsample audio from 44100Hz to 22050Hz, and compute the Short-Time Fourier Transform (STFT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is similar to NAF repository: https://github.com/aluo-x/Learning_Neural_Acoustic_Fields/blob/master/data_loading/data_maker.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path_name, use_torch=True, resample=False, resample_rate=22050, clip=True):\n",
    "    if use_torch:\n",
    "        loaded = torchaudio.load(path_name)\n",
    "        wave_data_loaded = loaded[0].numpy()\n",
    "        sr_loaded = loaded[1]\n",
    "    else:\n",
    "        loaded = wavfile.read(path_name)\n",
    "        if clip:\n",
    "            wave_data_loaded = np.clip(loaded[1], -1.0, 1.0).T\n",
    "        else:\n",
    "            wave_data_loaded = loaded[1].T\n",
    "        sr_loaded = loaded[0]\n",
    "\n",
    "    if resample:\n",
    "        if wave_data_loaded.shape[1]==0:\n",
    "            assert False\n",
    "        if wave_data_loaded.shape[1]<int(sr_loaded*0.1):\n",
    "            padded_wav = librosa.util.fix_length(wave_data_loaded, int(sr_loaded*0.1))\n",
    "            resampled_wave = librosa.resample(padded_wav, orig_sr=sr_loaded, target_sr=resample_rate)\n",
    "        else:\n",
    "            resampled_wave = librosa.resample(wave_data_loaded, orig_sr=sr_loaded, target_sr=resample_rate)\n",
    "    else:\n",
    "        resampled_wave = wave_data_loaded\n",
    "    \n",
    "    if clip:\n",
    "        return np.clip(resampled_wave, -1.0, 1.0)\n",
    "    else:\n",
    "        return resampled_wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_spec():\n",
    "    def __init__(self, fft_size=512):\n",
    "        self.n_fft=fft_size\n",
    "        self.hop = self.n_fft//4\n",
    "        self.spec_transform = Spectrogram(power=None, n_fft=self.n_fft, hop_length=self.hop)\n",
    "        \n",
    "    def transform(self, audio_data_prepad):\n",
    "        waveforms = librosa.util.fix_length(data=audio_data_prepad, size=audio_data_prepad.shape[-1]+self.n_fft//2)\n",
    "        if waveforms.shape[-1]<4410:\n",
    "            waveforms = librosa.util.fix_length(data=waveforms, size=4410)\n",
    "\n",
    "        transformed_data = self.spec_transform(torch.from_numpy(waveforms)).numpy()\n",
    "        \n",
    "        real_component = np.abs(transformed_data)\n",
    "\n",
    "        return real_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = 'office_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orientations = ['0', '90', '180', '270']\n",
    "spec_getter = get_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for rot in orientations:\n",
    "    print('Processing orientation', rot)\n",
    "    files_rot = sorted(os.listdir(os.path.join(scene, 'binaural_rirs', rot)))   \n",
    "    output_path_real = os.path.join(scene, 'binaural_magnitudes_sr22050', rot)\n",
    "\n",
    "    if not os.path.exists(output_path_real):\n",
    "        os.makedirs(output_path_real)\n",
    "\n",
    "    r_s_indexes = [elt.split('.')[0] for elt in files_rot] # receiver-source format\n",
    "\n",
    "    ff_count = 0\n",
    "    for r_s in r_s_indexes:\n",
    "        if ff_count % 500==0: # track progress\n",
    "            print('Processing', ff_count, 'out of', len(r_s_indexes))\n",
    "        \n",
    "        # Load SoundSpaces 1.0 binaural RIR\n",
    "        audio_file = r_s + '.wav'\n",
    "        audio_path = os.path.join(scene, 'binaural_rirs', rot, audio_file)\n",
    "        loaded_audios = load_audio(audio_path, use_torch = False, resample=True, clip=True) # same as NAF \n",
    "\n",
    "        # Compute Magnitude STFT\n",
    "        raw_real = spec_getter.transform(loaded_audios)\n",
    "\n",
    "        # Save Magnitude STFT\n",
    "        save_path = os.path.join(output_path_real, r_s + '.npy')\n",
    "        np.save(save_path, raw_real)\n",
    "\n",
    "        ff_count += 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
